
library(caret)
library(tidyverse)


## train test split
# step 1. spilt data
# 2. train 
# 3. score (predict)
# 4. evaluate

glimpse(mtcars)

## split data 80%:20%
#set.seed(42)
#(n <- nrow(mtcars))
#(id <- sample(1:n,size=0.8*n))
#train_data <- mtcars[id, ]
#test_data <- mtcars[-id, ]
# or dev finction 
train_test_split  <- function(data,trainratio=0.7) {
  set.seed(42)
  (n <- nrow(data))
  id <- sample(1:n,size = trainratio*n)
  train_data <- data[id, ]
  test_data <- data[-id, ]
  return(list(train=train_data,test=test_data))
}
set.seed(42)
splitdata <- train_test_split(mtcars,0.8)
train_data <- splitdata$train
test_data <- splitdata$test
# dev fuction คือจะทำไรก็แก้แค่ตรง mtcars

## train ตัวหนอนคือเป็นฟังก์ชั่นของ บวกคือเหมือน,ในสมการว่าแบบมีความสัมพันธ์กันยังไง
model <- lm(mpg ~ hp + wt + am, data = train_data)


#score  model 
mpg_pred <- predict(model,newdata = test_data)

#evaluate model (actual - prediction)
# find error by using MAE , MSE , RMSE
# โดยเฉลี่ยทายผิดไปประมาณเท่าไหร่ยิ่งค่าต่ำลงแสดงว่าแม่นยำมากขึ้น
mae_metric <- function(actual, prediction) {
  abs_error <- abs(actual - prediction)
  mean(abs_error)
}

## mae treats every data point the same
## mse treats ตัวที่ทายผิดเยอะๆ มากกว่า


mse_metric <- function( actual , prediction) {
  sq_error <- (actual - prediction)^2
  mean(sq_error)
}


rmse_metric <- function( actual , prediction) {
  sq_error <- (actual - prediction)^2
  sqrt(mean(sq_error))
}


## caret = classification and regression tree 
##Supervised learining = prediction

library(caret)

# 1. split data
splitData <- train_test_split(mtcars,0.7)
train_data <- splitData[[1]]
test_data <- splitData[[2]]
# 2. train model
set.seed(42)
#ctrl <- trainControl(
#  method = "cv",
#  number = 5,
#  verboseIter = TRUE
)
model <- train(mpg ~ hp + wt +am , data = train_data,method = "lm",trControl = ctrl)
# 3. score model
p <- predict(model,newdata = test_data)
# 4.evaluate model
rmse_metric(test_data$mpg,p)

# 5. save model 
saveRDS(model,"linear_regression_v1.RDS")


## on our friend laptop
## read model into R

(new_cars <- data.frame(
  hp = c(150,200,250),
  wt = c(1.25,2.2,2.5),
  am = c(0,1,1)
))

model <- readRDS("linear_regression_v1.RDS")

new_cars$mpg_pred <- predict(model,newdata = new_cars)
View(new_cars)






library(caret)
library(tidyverse)
library(mlbench)
library(MLmetrics)
library(forcats)
library(rpart.plot)

#see and glimpse data

data("BostonHousing")
df <- BostonHousing
glimpse(df)

## clustering => srgmentation

subset_df <- df %>%
  select(crim, rm , age , lstat , medv) %>%
  as_tibble()

#test different k (k=2-5)
result <- kmeans(x = subset_df, centers = 3)

#membership [1,2,3]
subset_df$cluster <- result$cluster


subset_df %>% 
  group_by(cluster) %>%
  summarise(avg_medv = mean(medv))
  
# distance between 2 point (x,y,or Z)
p1 <- c(2,3)
p2 <- c(6,8)
(p1-p2)^2
sqrt(sum((p1-p2)^2))
d <- sqrt(sum((p1-p2)^2))
## -------------------------------------
## lm, knn

df <- as_tibble(df)

# 1.split data
set.seed(42)
n <- nrow(df)
id <- sample(1:n,size =0.8*n)
train_data <- df[id,]
test_data <- df[-id,]

# metric คือยึดค่าไหนให้ดีสุดสามารถกำหนดได้
# set seed คือไม่ให้ model มันเปลี่ยนไปเปลี่ยนมา

# 2. train model
# medv = f (crim,rm,age) 
set.seed(42)
ctrl <- trainControl(method = "cv",
                     number = 5,
                     verboseIter = TRUE)
# grid search กำหนดค่า k ได้ tune grid ก็คือเอา k เท่ากำหนดมาทำ
k_grid <- data.frame(k = c(3,5,7,9,11))

knn_model <- train(medv ~ crim + rm + age,
                   data = train_data,
                   method = "knn",
                   metric = "Rsquared",
                   tuneGrid = k_grid,
                   preProcess = c("center","scale"),
                   trControl = ctrl )
## tuneLength ramdom search คือ เราหมุนหาค่า k ที่ได้ error น้อยที่สุด
knn_model <- train(medv ~ crim + rm + age,
                   data = train_data,
                   method = "knn",
                   metric = "Rsquared",
                   tuneLength = 5,
                   preProcess = c("center","scale"),
                   trControl = ctrl )

lm_model <- train(medv ~ crim + rm + age,
                   data = train_data,
                   method = "lm",
                   preProcrss = c("center","scale"))
# 3. score

p <- predict(knn_model,newdata = test_data)

# 4.evaluate
RMSE(p,test_data$medv)
## ค่า k คือมันแล้วแต่เราเลยแต่ว่าถ้าเลือกได้ถูกโมเดลก็จะทำงานได้ดี




#----------------------
#classification problem
data("PimaIndiansDiabetes")
glimpse(PimaIndiansDiabetes)

df <- PimaIndiansDiabetes
df$diabetes <- fct_relevel(df$diabetes, "pos")

subset_df <- df %>%
  select(glucose,insulin,age,diabetes)

train(diabetes ~ .,
      data = subset_df,
      method = "glm")

# 1. split data
set.seed(42)
n <- nrow(df)
id <- sample(1:n , size = 0.8*n)
train_data <- subset_df[id,]
test_data <- subset_df[-id,]
# 2. train data
set.seed(42)
ctrl <- trainControl(method = "cv",
                     number = 5,
                     verboseIter = TRUE,
                     summaryFunction = prSummary,
                     classProbs = TRUE)
knn_model <- train(diabetes ~ .,
                   data = train_data,
                   method = "knn",
                   metric = "Accuracy", 
                   trControl = ctrl)
# 3. score
p <- predict(knn_model,newdata = test_data)
# 4. evaluate
table(test_data$diabetes,p,dnn = c("Actual","Prediction"))
confusionMatrix(p,test_data$diabetes,
                positive ="pos",
                mode = "prec_recall")


#------- พวก summaryFunction มีไว้ตอนที่เราอยากคำนวณค่าอื่นที่ไม่ใช่ accu แต่ถ้าจะเอา accu ลบทิ้งได้เลย
## logistic regression
set.seed(42)
ctrl <- trainControl(method = "cv",
                     number = 5,
                     verboseIter = TRUE)
lgt_model <- train(diabetes ~ .,
                   data = train_data,
                   method = "glm",
                   metric = "Accuracy", 
                   trControl = ctrl)

## decision tree (rpart)
tree_model <- train(diabetes ~ .,
                   data = train_data,
                   method = "rpart",
                   metric = "Accuracy", 
                   trControl = ctrl)

rpart.plot(tree_model$finalModel)

## Random forest
## model accuracy the highest >= 70%
## mtry = number of feature used to train model
##bootstrap
##bagging technique(mtry+boot)
mtry_grid <- data.frame(mtry = 2:8)
rf_model <- train(diabetes ~ .,
                    data = train_data,
                    method = "rf",
                    metric = "Accuracy",
                    tuneGrid = mtry_grid,
                    trControl = ctrl)

# compare model

list_models <- list(knn = knn_model,
                    logistic = lgt_model,
                    decisiontree = tree_model,
                    randomforest = rf_model)

result <- resamples(list_models)
summary(result)


##-----------------------
## ridge vs lasso regression
library(glmnet)
# 0 = ridge 1= lasso
glmnet_grid = expand.grid(alpha = 0:1,
                         lambda = c(0.1,0.2,0.3))

glmnet_model <- train(diabetes ~ .,
                  data = train_data,
                  method = "glmnet",
                  metric = "Accuracy",
                  tuneGrid = glmnet_grid,
                  trControl = ctrl)
